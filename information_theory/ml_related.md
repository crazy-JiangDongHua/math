# 机器学习相关信息理论

![entropy_table](.\entropy_table.jpg)



## 信息熵(H(X,Y))

描述随机变量x,y联合分布P(X,Y)的不确定性。



## 相对熵(KL(X,Y))

也叫KL散度，KL散度是另一个用来衡量**分布相似度**的量，即从分布X到分布Y的 KL 散度为它们之间的变化所带来的信息增益，而不是“距离”（KL散度不具有交换性，衡量不是空间而是两个分布间的信息损失）。



## 条件熵(H(Y|X))

在随机变量x发生的前提下，随机变量y发生所新带来的熵定义为Y的条件熵，用H(Y|X)表示，用来衡量在已知随机变量X的条件下随机变量Y的不确定性。且此时H(Y|X) = H(X,Y) – H(X)成立。



## 交叉熵(H(X;Y))  

度量两个概率分布X,Y间的差异性信息，和相对熵只差一个常数，H(X;Y)-H(Y)=KL(X,Y)。



## 互信息(I(X,Y))

度量 X和 Y共享的信息，即度量知道这两个变量其中一个，对另一个不确定度减少的程度。所以互信息定义为X，Y的联合分布和各自独立分布乘积的相对熵，且此时I(X,Y)=KL(P(X,Y) , P(X)P(Y))。

有H(Y)-I(X,Y)=H(Y|X)，又有H(Y|X) = H(X,Y) – H(X)，所以有I(X,Y)= H(X) + H(Y) - H(X,Y)。



