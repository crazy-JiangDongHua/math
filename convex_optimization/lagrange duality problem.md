## 1、从最优化引出的拉格朗日乘数法

### 1.1 一个最简单的无约束优化问题

在学习与工程之中，我们时常会遇到一些优化的问题，也就是要对某个目标函数求取极值，最简单的形式如下。
$$
\min_{⁡x\in R^n}f(x) 
$$
这个式子表达的含义是以$x$为自变量，求取$f(x)$的最小值。如果想要求取最大值，可以把 $f$加一个负号，这样把$\max_{x}{f(x)}$转换为等价的$\min_{x}{-f(x)}$就行了。另外需要注意的是$x$并非一个标量数值，而是$n$维实数空间上的一个向量。

这样的例子有很多，最常见的就比如我们在机器学习中经常碰到的最小化代价函数。对于这样的问题，我们求解的方法很简单，只要能够求出一个点$x^\prime$ ，使得$\nabla f(x^\prime) = 0$即可。

当然我们也知道，只有当$f$是凸函数的时候，这样求出来的才是全局最优点，否则不过是个局部最优点罢了。不过这就不在我们的讨论范围内了。现在我们姑且假定讨论的$f$ 都是凸函数。

### 1.2 带有等式约束的优化问题

我们在上一节的基础上进一步，如果$f$是有约束的呢？
$$
\min_{x\in R^n} f(x)\hfill
\\s.t\quad h_i(x)=0,i\in[1,l]\hfill
$$
在这个问题中，我们对在定义域上的$x$有了$l$个等式约束$h_i$ 。这样一来我们就不能随意的计算导数等于0的点作为最优点了。那要怎么做呢？

这时就需要用到拉格朗日乘数法了，具体而言，我们首先来构建这么一个拉格朗日函数:
$$
\begin{align}
& L(x,\lambda)=f(x)+\sum_{i=1}^{l} \lambda_i h_i(x) \\
& x\in R^n, \lambda \in R^l 
\end{align}
$$
这里$\lambda$是新引入的参数，被称为拉格朗日乘子，在这里它是一个$l$维的向量。

接着我们需要解方程:
$$
\left\{
\begin{aligned}
& \frac{\partial L}{\partial x}=0 \\
& h_i(x)=0 
\end{aligned}
\right.
$$
求解出来的结果$x^\prime$必然就是最大值或最小值对应的点，只要稍加检验就可以得出结果了。

那么为什么会这样呢？这里我们稍加解释，如下图所示，$f(x, y)=x^2+y^2$是我们的目标函数$f(x)$，而$h(x, y)=xy-1=0$则是我们的约束条件$h(x)$。

首先我们画出$f(x, y)$的图像，这个图像应该是3维的，但为了方便讲解，这里给出它的2维投影：

![lagrange1](.\img\lagrange1.png)

图中的红色圆表示$f(x, y)$，越靠近原点的部分，值越小（表示“谷底”），这些圆又称为「等高线」，因为同一个圆代表的函数值相同。图中的蓝线代表$h(x, y)，$这里只取$h(x, y)=0$的部分。整幅图像可以想象成一个巨大的山谷，原点是谷底，而我们的任务是在蓝线表示的道路上，找到最低的位置。

那要如何找到这个最低点呢？注意，图中用橙色和黑色标记了两个点。如果我们走到了橙色这个位置，那么很明显，可以发现这个点肯定不是最低的，因为我们可以沿着蓝线继续往内部的圆走，当我们走到黑色这个点时，会发现没法再往里面走了，而且，这个时候如果继续沿蓝线走，我们的位置反而升高了，这时，我们基本可以认为：我们找到了在蓝线这个限制条件下的最低点。

那么橙色这个点和黑色这个点有什么本质区别呢？拉格朗日观察到，黑点位置，蓝线和圆是相切的，而橙点位置显然不满足这个性质。那相切是否是必然的呢？拉格朗日告诉我们，是的，一定是相切的。而这一点，正是拉格朗日乘子法的核心。

#### 1.2.1 梯度

在正式理解拉格朗日乘子法的原理之前，我们要回顾一下梯度的概念。

在数学里面，梯度指的是函数变化最快的方向。例如：在一元函数$f(x)$中，梯度只能沿$x$轴正方向或负方向，而在二元函数$f(x,y)$中，梯度则是一个二维向量 $(\frac{\partial f}{\partial x},\frac{\partial f}{\partial y})$。

现在，我们要用到梯度一个重要的性质：**梯度跟函数等高线是垂直的**。

证明需要用到一点极限的知识。

梯度的数学定义为：$\nabla f=(\frac{\partial f}{\partial x},\frac{\partial f}{\partial y})$。假设 $\Delta x$，$\Delta y$是两个极小的变化量，根据全微分的知识，可以得到：$f(x+\Delta x, y+\Delta y) \approx f(x, y)+\frac{\partial f}{\partial x}\Delta x + \frac{\partial f}{\partial y}\Delta y $，如果$(\Delta x, \Delta y)$是在等高线方向的增量，那么$f(x+\Delta x, y+\Delta y) \approx f(x, y)$，这意味着$\frac{\partial f}{\partial x}\Delta x + \frac{\partial f}{\partial y}\Delta y=0$，换句话说，向量$\nabla f$和向量$(\Delta x, \Delta y)$的内积为 0。所以，梯度和函数的等高线是垂直的。

#### 1.2.2 拉格朗日乘子法的几何认识

现在，我们来感性地认识一下，为什么拉格朗日认为相切才能找到最低点（只是感性认识，不添加任何数学推导）。

![lagrange2](.\img\lagrange2.png)

在橙点这个位置，由于两条曲线不相切，所以橙线的梯度（上图橙色箭头）和蓝线的切线（蓝色虚线）肯定不垂直。在这种情况下，蓝线的两个切线方向，必定有一个往函数高处走（与梯度的夹角小于 90 度），有一个往函数低处走（与梯度的夹角大于 90 度）。所以，在两条曲线相交时，我们肯定不在最低点或最高点的位置。

![lagrange3](.\img\lagrange3.png)

那么，反过来想，如果两条曲线相切（上图），那么在切点这个位置，蓝线的切线和橙线的梯度是垂直的，这个时候，蓝线的切线方向都指向橙线的等高线方向。换句话说，在切点的位置沿蓝线移动很小的一步，都相当于在橙线的等高线上移动，这个时候，可以认为函数值已经趋于稳定了。所以，我们认为这个点的值“可能”是最低（高）的（之后解释为什么是“可能“。另外，个人觉得拉格朗日乘子法最好用反证法从不相切的点入手思考，从相切的点思考总有点别扭）。

既然相切可以帮助我们找到最低点，那么接下来我们要研究的便是如何利用相切来找出最低点。

相切，意味着在切点的位置，两条曲线的等高线方向是平行的，考虑到梯度与等高线垂直， 我们可以用两条曲线的梯度平行来求出切点位置（最低点）。

因此，根据梯度平行，我们能够得到一个方程组：$\nabla f + \lambda \nabla h = 0$，其中$\lambda表$示一个标量，因为我们虽然能保证两个梯度平行，但不能保证它们的长度一样（或者方向相同）。在高维函数中，$\nabla f$表示的是函数在各个自变量方向的偏导。对于上面的例子，我们可以求出函数$f$和$h$的偏导，再根据方程组：
$$
\left\{
\begin{align}
& \frac{\partial f}{\partial x}+ \lambda \frac{\partial h}{\partial x}=0 \\     & \frac{\partial f}{\partial y}+ \lambda \frac{\partial h}{\partial y}=0 \\
& g(x,y)=0
\end{align}
\right.
$$


求出切点。由于总共有三个方程和三个未知数，一般都能找到解（也可能存在多个解或无解的情况，之后会简单讨论）。

在实际求解时，人们会使用一个统一的拉格朗日函数：$L(x,y,\lambda)=f(x,y)+\lambda h(x,y)$，令这个函数偏导为0，我们可以得到：
$$
\left\{
\begin{align}
& \frac{\partial L}{\partial x}=\frac{\partial f}{\partial x}+\lambda\frac{\partial h}{\partial x}=0 \\
& \frac{\partial L}{\partial y}=\frac{\partial f}{\partial y}+ \lambda \frac{\partial h}{\partial y}=0 \\
& \frac{\partial L}{\partial\lambda}=h(x,y)=0
\end{align}
\right.
$$
结果和上面的方程组是一样的。

#### 1.2.3 多个约束条件

多个约束条件和单个约束条件是一样的。如果是多个约束条件，那么这些约束函数肯定是相交的，否则无解。多个约束条件一般会把变量约束到一个更低维的空间，例如，下图中，紫色球面和黄色平面将变量约束到黑色线的位置。

![Lagrange4](.\img\Lagrange4.jpg)

求解过程和单个约束条件是一样的，我们定义一个新的拉格朗日函数：$L(x_1,\dots,x_n,\lambda_1,\dots,\lambda_l)=f(x_1,\dots,x_n)+\sum_{i=1}^l{\lambda_i h_i(x_1,\dots,x_n)}$，然后同样令这个函数的导数$\nabla L=0$，最后可以得到$(n+l)$个方程以及$(n+l)$个未知数，一般也能求解出来。

### 1.2.4 极值点

根据拉格朗日乘子法的定义，这是一种寻找极值的策略，换句话说，该方法并不能保证找到的一定是最低点或者最高点。事实上，它只是一种寻找极值点的过程，而且，拉格朗日乘子法找到的切点可能不只一个（也就是上面的方程组可能找到多个解），例如下图：

![lagrange5](.\img\lagrange5.jpg)

图中相切的点有两个，而红点的函数值明显比黑点小。事实上，要想判断找到的点是极低点还是极高点，我们需要将切点代入原函数再进行判断。

### 1.3 再加上不等式约束

然而实际生活中我们可能碰到的问题比上面的还要复杂，不仅仅是有等式约束，还可能会有不等式约束。现在假定$f(x)$，$c_i(x)$，$h_i (x)$都是定义在$R^n$上的连续可微函数，则考虑约束最优化问题:
$$
\tag{1}
\begin{align}
\min_{x \in R^n}\quad & f(x) \\
s.t\quad & c_i(x)\leq0,i\in[1,k] \\
& h_j(x)=0,j\in[1,l]	
\end{align}
$$
式(1)就是我们实际会遇到的优化问题的基本形式。如何求解这个优化问题也就是这篇文章主要要探讨的。关于具体如何求解，我们从第三节再开始讲其。第二节中，我们主要先把一些需要用到的数学概念理清了。

## 2、预备的数学知识

### 2.1 数学符号说明

#### 2.1.1 min/max符号
首先我们看一下式(a)，式子(a)的意思是对于函数$f(x)$，考察$x$（可以理解为改变$x$），来找到它的最小值。有时候在$\min$的下标的位置还会带上一些限制条件，比如式(b)中的$x \in D$，意思就是“考察在集合$D$中的$x$。
$$
\tag{a}
\min_x f(x)
$$
这个符号的含义在函数只有一个自变量$x$的时候倒不会出现什么混淆，但是当自变量不止一个的时候就容易弄混了。来我们来看看式子(b):

$$
\tag{b}
\max_{x\in D}f(x, \lambda)
$$
函数$f$是一个关于$x, \lambda$两个变量的函数。所以式子(b)的含义就是固定$\lambda$不变，改变$x$从而使得$f$ 的值最大。有时候可能会看到这样的格式:
$$
g(\lambda) = \max_{x\in D}f(x,\lambda)
$$
这个式子的意思是:

* $g(\lambda)$是一个关于$\lambda$的函数
* 当$\lambda = y$时，$g(\lambda)$的取值为：固定$f(x, \lambda)$中的$\lambda = y$，变动$x$时$f$能取到的最大值

再进一步，有时min还有max会像式(c)一样叠加在一起:
$$
\tag{c}
v = \max_{\lambda}\min_{x}f(x,\lambda)
$$
这个式子要从右往左看，首先把$\min_{x}f(x, \lambda)$这一项理解成关于$\lambda$的函数$g(\lambda)$，然后再对$g$求最大值，即:
$$
g(\lambda)=\min_x f(x,\lambda) \\
v = \max_\lambda g(\lambda)
$$

#### 2.1.2 其他符号

* $dom\ f$: 函数$f$的定义域
* $\cap S_i$：对若干集合$S_i$求交集

### 2.2 凸函数和凹函数

关于凸函数有性质如下:

若函数$f: R^n \to R$是凸的，则:
$$
\begin{align}
& \forall x, y\in dom\ f, \forall\theta\in[0,1] \\
& f(\theta x+(1−\theta)y) \leq \theta f(x)+(1−\theta)f(y)
\end{align}
$$
关于凹函数有性质如下:

若函数$f: R^n \to R$是凹的，则:
$$
\begin{align}
& \forall x, y\in dom\ f, \forall\theta\in[0,1] \\
& f(\theta x+(1−\theta)y) \geq \theta f(x)+(1−\theta)f(y)
\end{align}
$$

### 2.3 仿射函数

假如有$x \in R^n ,b \in R^m $ , 矩阵$A_{m\times n}$，那么:
$$
x \rightarrow Ax + b
$$
被称为$R^n \rightarrow R^m$仿射变换，这一过程被称为仿射函数。
$$
f(x) = Ax + b,\ x\in R^n
$$
比如最简单的:
$$
f(x) = a_1x_1 + a_2x_2 + \cdots + a_nx_n + b, x\in R^n
$$
就是一个$R^n \rightarrow R$仿射函数。

仿射函数有一个非常重要的性质，那就是它既凹又凸，准确来讲在凹凸函数的那个不等式中，仿射函数是可以取等号的:
$$
f(\theta x + (1 - \theta)y) = \theta f(x) + (1 - \theta) f(y)
$$

### 2.4 凸优化问题

我们在 1.3 节中的式 (1) 给出的式子其实是优化问题的基本形式:
$$
\tag{1}
\begin{align}
\min_{x \in R^n}\quad & f(x) \\
s.t\quad & c_i(x)\leq0,i\in[1,k] \\
& h_j(x)=0,j\in[1,l]	
\end{align}
$$
而所谓的凸优化问题，是满足一些特定条件的优化问题，它要求:

1. $f(x)$是凸函数
2. $c_i(x)$是凸函数
3. $h_j(x)$是仿射函数

凸优化有一个重要的性质：**任意位置的局部最优解同时也是全局最优解**。

### 2.5 凸二次规划问题

凸二次规划问题是凸优化问题的一个特殊形式，当目标函数是**二次型函数**且不等式约束函数$c(x)$是仿射函数时，就变成一个凸二次规划问题。**凸二次规划问题存在解**。例如：svm的优化问题。

## 3、从广义拉格朗日函数到拉格朗日对偶函数

### 3.1 拉格朗日乘数法的局限性

当存在不等式约束时，最优解的位置只有两种情况，一种是最优解在不等式约束的边界上，另一种就是不等式约束的区域内，需要分为两种情况讨论。例子：

**情况一：最优点在不等式约束的区域内**

原始问题为：
$$
\begin{align}
& \min_{x,y} f(x,y)=x^2+y^2 \\
& s.t. \quad c(x,y)=x+y-1\leq 0 
\end{align}
$$


画出函数图下如下：

![lagrange6](.\img\lagrange6.jpg)

与前文描述一致，红色为$f(x,y)$的等高线，越靠近原点的部分值越小。虚线表示$c(x,y)$，点阵覆盖的区域表示不等式约束的范围。可以看到，这个不等式约束实际上包含了原点，所以这个约束等于没有对于。这种情况，直接抛开约束条件，按照正常求极值办法，直接对$f(x,y)=x^2+y^2$求梯度，令其等于0，即可得到极值点。

**情况二：最优点在不等式的边界上**

原始问题为：
$$
\begin{align}
& \min_{x,y} f(x,y)=x^2+y^2 \\
& s.t. \quad c(x,y)=x+y+2\leq 0 
\end{align}
$$
画出函数图下如下：

![lagrange7](.\img\lagrange7.jpg)

可以看到，在不等式约束下，最优解是在边缘相切的地方取得，换句话说，最优解也在约束的边界上。在约束条件的边界上取得？稍等，**这不就直接是等式约束了吗**？那就直接套上面拉格朗日乘子法的方法来求呗。

**两种情况合并：**

那这两种情况，我们每次都需要分开讨论？那这也太拉了，我们现在把他们统一起来。为表示方便，后面用$x$替代$\vec x=(x,y)$。

- **情况一**：最优解在$c(x)<0$区域内，约束条件无作用，直接令$\nabla f=0$求解。这等价于将拉格朗日函数的$\lambda$置零，即$L(x,\lambda)=f(x)+\lambda c(x)=f(x)$，再对$f(x)$求极值。
- **情况二**：最优解在$c(x)=0$上，相当于等式约束，曲线相切处为最优解。即存在一个$\lambda$使得$\nabla L(x,\lambda)=\nabla f(x)+\lambda \nabla c(x)=0$，但是这里的$\lambda$的取值范围就不是不等于0了，这个时候最优解处$\nabla f$的方向必须与$\nabla c$的**相反**，即$\lambda >0$。

* **整合情况**：第一种$\lambda=0,c(x)<0$，第二种$\lambda>0,c(x)=0$，两种都有$\lambda c(x)=0$，所以合并转化为求解$\min L(x,\lambda)$问题的三个约束：
  $$
  \left\{
  \begin{align}
  & c(x) \leq 0 \\
  & \lambda \geq 0 \\
  & \lambda c(x) = 0
  \end{align}
  \right.
  $$

上述的是一个不等式约束的情况，可以推广到多个不等式约束。考虑前文提到的优化问题(1)
$$
\tag{1}\begin{align}\min_{x \in R^n}\quad & f(x) \\s.t\quad & c_i(x)\leq0,i\in[1,k] \\& h_j(x)=0,j\in[1,l]    \end{align}
$$
我们可以定义广义拉格朗日函数：
$$
\tag{2}
\begin{align}
&L(x,\lambda,\mu)=f(x)+\sum_{i=1}^{k}\lambda_ic_i(x)+\sum_{j=1}^{l}\mu_jh_j(x)
\\ & x \in R^n, \lambda \in R^k, \mu \in R^l,
\\ & L:R^n \times R^k \times R^l \to R
\end{align}
$$
其中$\lambda,\mu$被称为拉格朗日乘子。经过前文的分析，原问题可以转为求解下面的方程组：
$$
\left\{
\begin{align}
& \nabla_x L(x,\lambda,\mu) = 0 \tag{a} \\
& h_j(x) = 0 \tag{b} \\
& c_i(x) \leq 0 \tag{c} \\
& \lambda_i \geq 0 \tag{d} \\
& \lambda_i c_i(x) = 0 \tag{e} \\
\end{align}
\right.
$$
上述五个条件合在一起就是大名鼎鼎的KKT条件，理解一下就是：

* a：约束曲面与函数等高线相切，梯度方向平行
* b：等式约束
* c：不等式约束
* d：前文推导的不等式约束的两种情况下，$\lambda$满足的条件
* e：前文推导的不等式约束的两种情况下，都满足$\lambda c(x)=0$，称为松弛互补条件

原问题的可行解必然满足KKT条件，那就说明**KKT条件是最优解的必要条件**。对于凸优化问题，我们知道任何局部最优解一定是全局最优解，**所以满足KKT条件的解，一定是最优解（充要条件）**。

但是，这个方程组还是很难求啊！！！还是有超级多的约束条件，一般都求不出来欸，没办法，只能请出拉格朗日对偶函数来帮忙了。

### 3.2 啥是对偶

OK，做了一大堆铺垫，我们终于要开始正式将拉格朗日对偶了。首先来回忆一下我们的原问题 (Primal Problem):
$$
\tag{1}\begin{align}\min_{x \in R^n}\quad & f(x) \\s.t\quad & c_i(x)\leq0,i\in[1,k] \\& h_j(x)=0,j\in[1,l]    \end{align}
$$
首先我们做两个约定:

* 我们**不假定原函数 *f* 的凹凸性**，也就是 *f* 可以是非凸非凹函数
* 我们约定最终求出来的最优结果用$p^*$表示

对于原问题(1)我们通常用拉格朗日对偶的方式来求解。啥是对偶？对偶说白了就是实质相同但从不同角度提出不同提法的一对问题。有时候原问题 (Primal Problem) 不太好解，但是对偶问题 (Dual Problem) 却很好解，我们就可以通过求解对偶问题来迂回地解答原问题。

在这里我们用的是拉格朗日对偶的方法来解决，既然我们说用对偶的方法的原因在于原问题不好解，那么这里也是如此吗？当然了。首先我们来看看原问题有什么难的地方:

* 约束条件太多：很显然约束越多，问题就越难解决，原问题中总共有$k + l$个约束，相当麻烦。如果用KKT条件约束更多......
  
* 原问题凹凸性不明确：之前我们说过，不假定原函数 *f* 的凹凸性，这就意味着我们无法将凸优化的方法应用在原问题中

那么它的拉格朗日对偶问题有什么优点呢？

* 只有一个约束

* **拉格朗日对偶问题一定是凹的**

这里先做一个感性的认识，细节方面接下来慢慢说。

#### 3.2.1 原始问题

我们首先引入前文提到的广义拉格朗日函数：
$$
\tag{2}
\begin{align}
&L(x,\lambda,\mu)=f(x)+\sum_{i=1}^{k}\lambda_ic_i(x)+\sum_{j=1}^{l}\mu_jh_j(x)
\\ & x \in R^n, \lambda \in R^k, \mu \in R^l,
\\ & L:R^n \times R^k \times R^l \to R
\end{align}
$$
构建关于$x$的函数：
$$
\theta_P(x) = \max_{\lambda,\mu,\lambda_i\geq 0}L(x,\lambda,\mu)
$$
假设给定某个违法原始问题约束条件的解$x$，即存在某个$i$使得$c_i(x)>0$或$h_j(x)\ne0$。若$c_i(x)>0$，可以使得$\lambda_i=+\infty$，使得$\theta_P(x)=+\infty$；若$h_j(x)\ne0$，可以使得$\mu_j h_j(x)=+\infty$。并将其余$\lambda,\mu$取为0

假设给定某个符合原始问题约束条件的解$x$，即$c_i(x)\leq0$和$h_j(x)=0$。则$\sum_{i=1}^{k}\lambda_ic_i(x)+\sum_{j=1}^{l}\mu_jh_j(x)\leq 0$，所以有$\theta_P(x)=f(x)$。

综上有：
$$
\theta_P(x) =
\left\{
\begin{align}
& f(x),\ x满足原始问题约束 \\
& +\infty,\ else
\end{align}
\right.
$$
那么就说明，极小化$\theta_P(x)$与原始问题等价，有相同的解（因为当趋向无穷时，问题无解，因此得到的解必须满足约束条件），即
$$
\min_x \theta_P(x) = \min_x \max_{\lambda,\mu,\lambda_i\geq 0}L(x,\lambda,\mu)
$$
该问题也被称为广义拉格朗日函数的极小极大问题。

原始问题的最优值为：
$$
p^* = \min_x \theta_P(x)
$$

#### 3.2.2 对偶问题

构建关于$\lambda,\mu$的函数：
$$
\theta_D(\lambda,\mu) = \min_{x}L(x,\lambda,\mu)
$$
极大化问题：
$$
\max_{\lambda,\mu,\lambda_i\geq 0} \theta_D(\lambda,\mu) = \max_{\lambda,\mu,\lambda_i\geq 0} \min_{x}L(x,\lambda,\mu)
$$
称为广义拉格朗日函数的极大极小问题。把问题写成约束优化问题的形式：
$$
\tag{3}
\begin{align}
&\max_{\lambda,\mu} \theta_D(\lambda,\mu) = \max_{\lambda,\mu} \min_{x}L(x,\lambda,\mu) \\
& s.t. \quad\lambda_i\geq 0, \ i\in [1,k]
\end{align}
$$
该问题也称为原始问题的对偶问题。

定义对偶问题的最优值
$$
d^* = \max_{\lambda,\mu,\lambda_i\geq 0} \theta_D(\lambda,\mu)
$$


### 3.3 对偶函数的凹性

对偶函数$g(\lambda,\mu)=\theta_D(\lambda,\mu) = \min_x L(x,\lambda,\mu)$，有一个非常重要的性质：**它一定是一个凹函数**，我们可以证明一下。首先我们明确一个凹函数具有性质:
$$
f(\theta x+(1−\theta)y) \geq \theta f(x)+(1−\theta)f(y)
$$
要证明 $g$ 是一个凹函数就是要证明它满足这个不等式。

1. 我们首先记：$g(\lambda,\mu) = min_x\{L(x_1,\lambda,\mu),\dots,L(x_n,\lambda,\mu)\},n \to +\infty$。这个就是说把连续的函数离散成无限个点的集合，这样就可以把 $g$ 看作一个逐点求最小的无穷集合。

2. 简记:$\gamma=(\lambda,\mu),g(\gamma)=g(\lambda,\mu)$，方便书写

3. 证明过程如下：
   $$
   \begin{align}
   & g(\theta \gamma_1 + (1-\theta)\gamma_2) \\
   & = \min\{L(x_1,\theta \gamma_1 + (1-\theta)\gamma_2),\dots,L(x_n,\theta \gamma_1 + (1-\theta)\gamma_2)\} \\
   & = \min\{\theta L(x_1, \gamma_1),\dots,\theta L(x_n, \gamma_1), 
   (1-\theta)L(x_1, \gamma_1),\dots,(1-\theta)L(x_n, \gamma_1)\} \\
   & \geq \theta \min \{L(x_1, \gamma_1),\dots, L(x_n, \gamma_1)\} + (1-\theta)\{L(x_1, \gamma_2), \dots, L(x_n, \gamma_2)\} \\
   & = \theta g(x, \gamma_1) + (1-\theta) g(x, \gamma_2)
   \end{align}
   $$
   来逐行解释一下:

   - 第一行：之前讲过了，略
   - 第二行：因为$x_i$确定了，所以$f(x),c_i(x),h_j(x)$都是常数，所以$L(x_i,\gamma)$就成了一个仿射函数，仿射函数又凹又凸，对于判定凹凸函数的不等式可以取等号。
   - 第三行：使用了一个简单数学原理：$\min(a+b)\geq \min(a)+\min(b)$

所以，**不管原函数 $f$ 的凹凸性，它的对偶函数 $g$ 一定是凹函数**

### 3.4 原始问题和对偶问题的关系

约束问题和这里的对偶性问题是怎么联系起来的呢？通过下面的关系就很容易发现


$$
d^\ast = \max_{\lambda,\mu,\lambda_i\geq 0} \min_{x}L(x,\lambda,\mu) \leq  \min_x \max_{\lambda,\mu,\lambda_i\geq 0}L(x,\lambda,\mu)  =p^\ast
$$

证明过程非常简单，因为
$$
\theta_D(\lambda,\mu) = \min_{x}L(x,\lambda,\mu) \leq L(x,\lambda,\mu) \leq
\max_{\lambda,\mu,\lambda_i\geq 0}L(x,\lambda,\mu) = \theta_P(x)
$$
所以有
$$
d^\ast =\max_{\lambda,\mu,\lambda_i\geq 0} \min_{x}L(x,\lambda,\mu)= \max_{\lambda,\mu,\lambda_i\geq 0}\theta_D(\lambda,\mu) \leq \min_x\theta_P(x)= \min_x \max_{\lambda,\mu,\lambda_i\geq 0}L(x,\lambda,\mu)  =p^\ast
$$
所以对偶问题给出了原问题的下界！这有啥好处嘛？我们来捋一捋:

1. 首先我们明确一件事，我们的目的是找到最优解$p^*$
2. 有时候 $p^*$其实并不一定能解出来，这种情况下，我们希望可以给出一个尽可能地逼近$p^*$ 的值
3. 既然我们已经知道了 $g$ 可以给出下界，那么那个值能够尽可能逼近呢？
4. 答案是$d^* = \max g,(s.t. \lambda_i\geq0)$

![lagrange8](.\img\lagrange8.png)

现在回顾一下之前说的求解对偶问题的好处，我们再来看看为什么需要拉格朗日对偶。

首先是问题 (1) 有哪些 annoying 的地方:

1. 原问题的约束太多了，又是等式又是不等式

2. 问题 (1) 不一定是一个凸优化问题，所以即便找到了貌似是$p^*$的点，也很可能不过是个局部最优点。

那Lagrange 对偶问题 (3) 就好一些吗？还真是，我们来康康:

1. 约束少了，这是很明显的，少了 $l$ 个，而且剩下的 $k$ 个约束也比原来的简单一些
2. 最最重要的，**对偶问题一定是一个凸优化问题**，所以很多凸优化的手段全可以用上了

同时我们也可以给出**推论(1)**：

**设$x^*$和$\lambda^*,\mu^*$分别是原始问题(1)和对偶问题(3)的可行解，并且$d^*=p^*$，那么$x^*$和$\lambda^*,\mu^*$分别是原始问题(1)和对偶问题(3)的最优解。**

## 4、弱对偶和强对偶

所谓强弱对偶，指的是拉格朗日对偶问题的一种性质，关于弱对偶我们其实在前面已经谈到了，即
$$
d^* \leq p^*
$$
即便原文题不是凸问题，上述的不等式也成立，这就是所谓的**弱对偶性 (Weak Dulity)**。这样的性质即便是$d^*,\ p^*$无限时也成立，也就是说:

* 如果$p^* = -\infty$，则$d^* = -\infty$
* 如果$d^* = +\infty$，则$p^* = +\infty$

我们还把$p^* - d^*$称为最优对偶间隙 (Optimal Duality Gap)，这个值必然非负。

但是还有一种很特殊的情况：
$$
d^*=p^*
$$
这种情况称为**强对偶性(Strong Duality)**。

![lagrange9](.\img\lagrange9.png)

之所以称之为“强”对偶，正是因为这种性质相对于“弱”对偶而言对我们的意义更加重大。很显然，当我们待求解的对偶问题是强对偶的话，那么对偶问题就等价于原始问题。

## 6、强对偶的条件

如果**原始问题是凸优化问题，且满足Slater条件**，那么一定满足强对偶（这是一个充分条件）。

下面解释以下具体含义，凸优化问题在2.4节提过，这意味着：

1. $f(x)$是凸函数
2. $c_i(x)$是凸函数
3. $h_j(x)$是仿射函数

所谓的 Slater 条件。严格的 Slater 条件表述如下:
$$
\exist x\ c_i(x)<0,\ i\in[1,k]
$$
我们把满足该条件的点称为是**严格可行**的，因为不等式约束要求严格成立。之所以称上述的 Slater 条件是严格的，是因为 Slater 还有一个弱化的版本。
$$
\begin{align}
\exist x\ & c_i(x)\leq 0,\ i\in[1,p] \\
& c_i(x)< 0,\ i\in[p+1,k] \\
s.t.\quad & (x)是仿射函数,\ i\in[i,p]
\end{align}
$$
任意满足:

1. 凸优化
2. 强或弱 Slater 条件

的优化问题同样满足强对偶性 (充分条件)。我们给出**推论(2)**：

**若原始问题(1)为凸优化问题，且满足强或弱Slater条件，那么一定存在$x^*$和$\lambda^*,\mu^*$分别是原始问题(1)和对偶问题(3)的解，并且$d^*=p^*=L(x^*,\lambda^*,\mu^*)$ 。**

**根据推论(1)，可知。$x^*$和$\lambda^*,\mu^*$分别为原始问题(1)和对偶问题(3)的最优解**

那这组最优解有什么性质嘛？回顾一下，我们之前提到KKT条件。对于原问题的最优解，一定满足KKT条件。当对偶问题为强对偶时，对偶问题等价于原始问题，所有最优解都要满足KKT条件。而对于凸优化问题，我们知道满足KKT条件的一定是最优解。所以可以得到**推论(3)**：

**若原始问题(1)为凸优化问题，且满足强或弱Slater条件，则$x^*$和$\lambda^*,\mu^*$分别为原始问题(1)和对偶问题(3)的最优解的充分必要条件是$x^*$和$\lambda^*,\mu^*$满足KKT条件**



## 参考

1. https://blog.csdn.net/frostime/article/details/90291392
1. https://jermmy.github.io/2017/07/27/2017-7-27-understand-lagrange-multiplier/
1. https://zhuanlan.zhihu.com/p/154517678
1. https://zhuanlan.zhihu.com/p/55532322
2. 统计机器学习